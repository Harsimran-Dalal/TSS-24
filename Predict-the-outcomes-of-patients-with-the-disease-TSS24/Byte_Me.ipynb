{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81971,"databundleVersionId":8916601,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harsimransinghdalal/byte-me?scriptVersionId=185381292\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport optuna\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:19:09.201403Z","iopub.execute_input":"2024-06-25T10:19:09.201868Z","iopub.status.idle":"2024-06-25T10:19:09.217879Z","shell.execute_reply.started":"2024-06-25T10:19:09.201834Z","shell.execute_reply":"2024-06-25T10:19:09.216608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/train.csv')\ntest_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/sample_submission.csv')\n\n# Display the first few rows of each dataframe to understand their structure\nprint(\"Train DataFrame Head:\")\nprint(train_df.head())\n\nprint(\"\\nTest DataFrame Head:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission DataFrame Head:\")\nprint(sample_submission_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:19:12.46089Z","iopub.execute_input":"2024-06-25T10:19:12.461292Z","iopub.status.idle":"2024-06-25T10:19:12.552764Z","shell.execute_reply.started":"2024-06-25T10:19:12.461262Z","shell.execute_reply":"2024-06-25T10:19:12.551511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom scipy.stats import randint\n\n# Custom transformer for outlier treatment\nclass OutlierTreatment(BaseEstimator, TransformerMixin):\n    def __init__(self, factor=1.5):\n        self.factor = factor\n\n    def fit(self, X, y=None):\n        if isinstance(X, pd.DataFrame):\n            self.Q1 = X.quantile(0.25)\n            self.Q3 = X.quantile(0.75)\n        else:  # Assuming it's a numpy array\n            self.Q1 = np.quantile(X, 0.25, axis=0)\n            self.Q3 = np.quantile(X, 0.75, axis=0)\n        self.IQR = self.Q3 - self.Q1\n        return self\n\n    def transform(self, X):\n        X_out = X.copy()\n        lower_bound = self.Q1 - self.factor * self.IQR\n        upper_bound = self.Q3 + self.factor * self.IQR\n        X_out = np.clip(X_out, lower_bound, upper_bound)\n        return X_out\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/train.csv')\ntest_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024/sample_submission.csv')\n\n# Identify features and target\nfeatures = train_df.drop(columns=['id', 'Status'])\ntarget = train_df['Status']\n\n# Split categorical and numerical features\nnumerical_features = features.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = features.select_dtypes(include=['object']).columns\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('outlier', OutlierTreatment(factor=1.5)),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Create the preprocessing and training pipeline\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                        ('classifier', RandomForestClassifier(random_state=42))])\n\n# Split the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Define the hyperparameters to tune\nparam_dist = {\n    'classifier__n_estimators': randint(100, 1000),\n    'classifier__max_depth': [10, 20, 30, 40, 50, None],\n    'classifier__min_samples_split': randint(2, 20),\n    'classifier__min_samples_leaf': randint(1, 20),\n    'preprocessor__num__outlier__factor': [1.0, 1.5, 2.0]\n}\n\n# Perform randomized search\nn_iter_search = 10  # Reduced number of iterations\nrandom_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5, scoring='neg_log_loss', n_jobs=-1)\n\nrandom_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = random_search.best_params_\nprint(f\"Best Hyperparameters: {best_params}\")\n\n# Fit the model with the best hyperparameters\nbest_model = random_search.best_estimator_\nbest_model.fit(X_train, y_train)\n\n# Validate the model\ny_valid_pred = best_model.predict_proba(X_valid)\n\n# Calculate log loss on the validation set\nlogloss = log_loss(pd.get_dummies(y_valid), y_valid_pred)\nprint(f\"Validation Log Loss with Best Model: {logloss}\")\n\n# Predict on the test data\ntest_pred = best_model.predict_proba(test_df.drop(columns=['id']))\n\n# Prepare the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'Status_C': [proba[0] for proba in test_pred],\n    'Status_CL': [proba[1] for proba in test_pred],\n    'Status_D': [proba[2] for proba in test_pred]\n})\n\n# Save the submission DataFrame to a CSV file\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:27:54.38186Z","iopub.execute_input":"2024-06-25T10:27:54.382265Z"},"trusted":true},"execution_count":null,"outputs":[]}]}